{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introdu_doc = \"\"\"N early ten years had passed since the Dursleys had woken up to find their\n",
    "# nephew on the front step, but Privet Drive had hardly changed at all. The sun\n",
    "# rose on the same tidy front gardens and lit up the brass number four on the\n",
    "# Dursleys’ front door; it crept into their living room, which was almost exactly\n",
    "# the same as it had been on the night when Mr. Dursley had seen that fateful news\n",
    "# report about the owls. Only the photographs on the mantelpiece really showed\n",
    "# how much time had passed. Ten years ago, there had been lots of pictures of\n",
    "# what looked like a large pink beach ball wearing different-colored bonnets —\n",
    "# but Dudley Dursley was no longer a baby, and now the photographs showed a\n",
    "# large blond boy riding his first bicycle, on a carousel at the fair, playing a\n",
    "# computer game with his father, being hugged and kissed by his mother. The\n",
    "# room held no sign at all that another boy lived in the house, too.\n",
    "# Yet Harry Potter was still there, asleep at the moment, but not for long.\n",
    "# His Aunt Petunia was awake and it was her shrill voice that made the first noise\n",
    "# of the day.\"\"\"\n",
    "introdu_doc = \"\"\"N early ten years had passed since the Dursleys had woken up to find their of the day.\"\"\"\n",
    "\n",
    "introdu_doc = introdu_doc.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = introdu_doc.split()\n",
    "unq_Words = sorted(list(set(tokens)))\n",
    "dictMarkup = {a:i for i,a in enumerate(unq_Words)}\n",
    "oneHotEncode = np.zeros((len(tokens), len(unq_Words)), dtype=int)\n",
    "for i, a in enumerate(tokens):\n",
    "    oneHotEncode[i, dictMarkup[a]] = 1\n",
    "\n",
    "oneHotEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words\n",
    "\n",
    "Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\",\n",
    "              \"Is this the first document?\"]\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    " \n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:\n",
    "\n",
    "Term Frequency (TF): Term Frequency measures how often a term (word) appears in a document. It is calculated using the formula:\n",
    "\\text{TF}(t,d) = \\frac{\\text{Total number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}          \n",
    "\n",
    "Inverse Document Frequency (IDF): Inverse Document Frequency measures the importance of a term across a collection of documents. It is calculated using the formula:\n",
    "\\text{IDF}(t,D) = \\log\\left(\\frac{\\text{Total documents }}{\\text{Number of documents containing term t}}\\right)          \n",
    "\n",
    "The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:\n",
    "\n",
    "TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)          \n",
    "\n",
    "The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "the: 0.6030226891555273\n",
      "quick: 0.30151134457776363\n",
      "brown: 0.30151134457776363\n",
      "fox: 0.30151134457776363\n",
      "jumps: 0.30151134457776363\n",
      "over: 0.30151134457776363\n",
      "lazy: 0.30151134457776363\n",
      "dog: 0.30151134457776363\n",
      "\n",
      "\n",
      "Document 2:\n",
      "journey: 0.3535533905932738\n",
      "of: 0.3535533905932738\n",
      "thousand: 0.3535533905932738\n",
      "miles: 0.3535533905932738\n",
      "begins: 0.3535533905932738\n",
      "with: 0.3535533905932738\n",
      "single: 0.3535533905932738\n",
      "step: 0.3535533905932738\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# Sample\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "]\n",
    " \n",
    "vectorizer = TfidfVectorizer()  # Create the TF-IDF vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = {}\n",
    " \n",
    "for doc_index, doc in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[doc_index, :].nonzero()[1]\n",
    "    tfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n",
    "    tfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values}\n",
    "#let's print\n",
    "for doc_index, values in tfidf_values.items():\n",
    "    print(f\"Document {doc_index + 1}:\")\n",
    "    for word, tfidf_value in values.items():\n",
    "        print(f\"{word}: {tfidf_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 14\n",
      "The words in the corpus:\n",
      " {'is', 'of', 'the', 'data', 'one', 'analyze', 'best', 'scientists', 'this', 'fields', 'science', 'courses', 'important', 'most'}\n",
      "Term Frequency (TF):\n",
      "         is        of       the      data       one  analyze      best  \\\n",
      "0  0.090909  0.181818  0.090909  0.090909  0.090909     0.00  0.000000   \n",
      "1  0.111111  0.111111  0.111111  0.111111  0.111111     0.00  0.111111   \n",
      "2  0.000000  0.000000  0.000000  0.500000  0.000000     0.25  0.000000   \n",
      "\n",
      "   scientists      this    fields   science   courses  important      most  \n",
      "0        0.00  0.000000  0.090909  0.181818  0.000000   0.090909  0.090909  \n",
      "1        0.00  0.111111  0.000000  0.111111  0.111111   0.000000  0.000000  \n",
      "2        0.25  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "             is: 0.17609125905568124\n",
      "             of: 0.17609125905568124\n",
      "            the: 0.17609125905568124\n",
      "           data:        0.0\n",
      "            one: 0.17609125905568124\n",
      "        analyze: 0.47712125471966244\n",
      "           best: 0.47712125471966244\n",
      "     scientists: 0.47712125471966244\n",
      "           this: 0.47712125471966244\n",
      "         fields: 0.47712125471966244\n",
      "        science: 0.17609125905568124\n",
      "        courses: 0.47712125471966244\n",
      "      important: 0.47712125471966244\n",
      "           most: 0.47712125471966244\n",
      "\n",
      "TF-IDF:\n",
      "         is        of       the  data       one  analyze      best  \\\n",
      "0  0.016008  0.032017  0.016008   0.0  0.016008  0.00000  0.000000   \n",
      "1  0.019566  0.019566  0.019566   0.0  0.019566  0.00000  0.053013   \n",
      "2  0.000000  0.000000  0.000000   0.0  0.000000  0.11928  0.000000   \n",
      "\n",
      "   scientists      this    fields   science   courses  important      most  \n",
      "0     0.00000  0.000000  0.043375  0.032017  0.000000   0.043375  0.043375  \n",
      "1     0.00000  0.053013  0.000000  0.019566  0.053013   0.000000  0.000000  \n",
      "2     0.11928  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19833/1472978829.py:30: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_tf[w][i] += 1 / len(words)\n",
      "/tmp/ipykernel_19833/1472978829.py:56: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Construct a small corpus\n",
    "corpus = [\n",
    "    \"data science is one of the most important fields of science\",\n",
    "    \"this is one of the best data science courses\",\n",
    "    \"data scientists analyze data\"\n",
    "]\n",
    "\n",
    "\n",
    "# Create a set of all unique words in the corpus\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split()\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number of words in the corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "n_docs = len(corpus)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Create a DataFrame to store TF values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
    "\n",
    "# Calculate TF for each word in each document\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split()\n",
    "    for w in words:\n",
    "        # Calculate TF as the number of occurrences of the word divided by the total number of words in the document\n",
    "        df_tf[w][i] += 1 / len(words)\n",
    "\n",
    "print(\"Term Frequency (TF):\")\n",
    "print(df_tf)\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "# Calculate IDF for each word\n",
    "for w in words_set:\n",
    "    k = 0  # Number of documents containing the word\n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "    # Calculate IDF using the log of the number of documents divided by the number of documents containing the word\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "    print(f\"{w:>15}: {idf[w]:>10}\")\n",
    "\n",
    "# Compute TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "# Calculate TF-IDF for each word in each document\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the pre-trained Word2Vec model from Google (takes some time)\n",
    "# This downloads a large file, so be patient.\n",
    "word2vec_model_path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "\n",
    "# Load the Word2Vec model using gensim\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n",
    "\n",
    "# Example: Get the word embedding for the word \"king\"\n",
    "word_embedding = word2vec_model[\"king\"]\n",
    "\n",
    "# Print the dimensionality of the word embedding\n",
    "print(\"Dimensionality of word embedding:\", len(word_embedding))\n",
    "\n",
    "# Example: Get the most similar words to \"king\"\n",
    "similar_words = word2vec_model.most_similar(\"king\")\n",
    "print(\"Words most similar to 'king':\", similar_words)\n",
    "\n",
    "# Example: Calculate the similarity between two words\n",
    "similarity_score = word2vec_model.similarity(\"king\", \"queen\")\n",
    "print(\"Similarity between 'king' and 'queen':\", similarity_score)\n",
    "\n",
    "# Example: Calculate the vector representing the combination of words \"king\" and \"man\" minus \"woman\"\n",
    "result_vector = word2vec_model.most_similar(positive=[\"king\", \"man\"], negative=[\"woman\"], topn=1)\n",
    "print(\"Vector representation of 'king - man + woman':\", result_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
