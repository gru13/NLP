{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introdu_doc = \"\"\"N early ten years had passed since the Dursleys had woken up to find their\n",
    "# nephew on the front step, but Privet Drive had hardly changed at all. The sun\n",
    "# rose on the same tidy front gardens and lit up the brass number four on the\n",
    "# Dursleys’ front door; it crept into their living room, which was almost exactly\n",
    "# the same as it had been on the night when Mr. Dursley had seen that fateful news\n",
    "# report about the owls. Only the photographs on the mantelpiece really showed\n",
    "# how much time had passed. Ten years ago, there had been lots of pictures of\n",
    "# what looked like a large pink beach ball wearing different-colored bonnets —\n",
    "# but Dudley Dursley was no longer a baby, and now the photographs showed a\n",
    "# large blond boy riding his first bicycle, on a carousel at the fair, playing a\n",
    "# computer game with his father, being hugged and kissed by his mother. The\n",
    "# room held no sign at all that another boy lived in the house, too.\n",
    "# Yet Harry Potter was still there, asleep at the moment, but not for long.\n",
    "# His Aunt Petunia was awake and it was her shrill voice that made the first noise\n",
    "# of the day.\"\"\"\n",
    "introdu_doc = \"\"\"N early ten years had passed since the Dursleys had woken up to find their of the day.\"\"\"\n",
    "\n",
    "introdu_doc = introdu_doc.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding\n",
    "\n",
    "One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = introdu_doc.split()\n",
    "unq_Words = sorted(list(set(tokens)))\n",
    "dictMarkup = {a:i for i,a in enumerate(unq_Words)}\n",
    "oneHotEncode = np.zeros((len(tokens), len(unq_Words)), dtype=int)\n",
    "for i, a in enumerate(tokens):\n",
    "    oneHotEncode[i, dictMarkup[a]] = 1\n",
    "\n",
    "oneHotEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words\n",
    "\n",
    "Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\",\n",
    "              \"Is this the first document?\"]\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    " \n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:\n",
    "\n",
    "Term Frequency (TF): Term Frequency measures how often a term (word) appears in a document. It is calculated using the formula:\n",
    "\\text{TF}(t,d) = \\frac{\\text{Total number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}          \n",
    "\n",
    "Inverse Document Frequency (IDF): Inverse Document Frequency measures the importance of a term across a collection of documents. It is calculated using the formula:\n",
    "\\text{IDF}(t,D) = \\log\\left(\\frac{\\text{Total documents }}{\\text{Number of documents containing term t}}\\right)          \n",
    "\n",
    "The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:\n",
    "\n",
    "TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)          \n",
    "\n",
    "The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "the: 0.6030226891555273\n",
      "quick: 0.30151134457776363\n",
      "brown: 0.30151134457776363\n",
      "fox: 0.30151134457776363\n",
      "jumps: 0.30151134457776363\n",
      "over: 0.30151134457776363\n",
      "lazy: 0.30151134457776363\n",
      "dog: 0.30151134457776363\n",
      "\n",
      "\n",
      "Document 2:\n",
      "journey: 0.3535533905932738\n",
      "of: 0.3535533905932738\n",
      "thousand: 0.3535533905932738\n",
      "miles: 0.3535533905932738\n",
      "begins: 0.3535533905932738\n",
      "with: 0.3535533905932738\n",
      "single: 0.3535533905932738\n",
      "step: 0.3535533905932738\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# Sample\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "]\n",
    " \n",
    "vectorizer = TfidfVectorizer()  # Create the TF-IDF vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = {}\n",
    " \n",
    "for doc_index, doc in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[doc_index, :].nonzero()[1]\n",
    "    tfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n",
    "    tfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values}\n",
    "#let's print\n",
    "for doc_index, values in tfidf_values.items():\n",
    "    print(f\"Document {doc_index + 1}:\")\n",
    "    for word, tfidf_value in values.items():\n",
    "        print(f\"{word}: {tfidf_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 14\n",
      "The words in the corpus:\n",
      " {'is', 'of', 'the', 'data', 'one', 'analyze', 'best', 'scientists', 'this', 'fields', 'science', 'courses', 'important', 'most'}\n",
      "Term Frequency (TF):\n",
      "         is        of       the      data       one  analyze      best  \\\n",
      "0  0.090909  0.181818  0.090909  0.090909  0.090909     0.00  0.000000   \n",
      "1  0.111111  0.111111  0.111111  0.111111  0.111111     0.00  0.111111   \n",
      "2  0.000000  0.000000  0.000000  0.500000  0.000000     0.25  0.000000   \n",
      "\n",
      "   scientists      this    fields   science   courses  important      most  \n",
      "0        0.00  0.000000  0.090909  0.181818  0.000000   0.090909  0.090909  \n",
      "1        0.00  0.111111  0.000000  0.111111  0.111111   0.000000  0.000000  \n",
      "2        0.25  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "             is: 0.17609125905568124\n",
      "             of: 0.17609125905568124\n",
      "            the: 0.17609125905568124\n",
      "           data:        0.0\n",
      "            one: 0.17609125905568124\n",
      "        analyze: 0.47712125471966244\n",
      "           best: 0.47712125471966244\n",
      "     scientists: 0.47712125471966244\n",
      "           this: 0.47712125471966244\n",
      "         fields: 0.47712125471966244\n",
      "        science: 0.17609125905568124\n",
      "        courses: 0.47712125471966244\n",
      "      important: 0.47712125471966244\n",
      "           most: 0.47712125471966244\n",
      "\n",
      "TF-IDF:\n",
      "         is        of       the  data       one  analyze      best  \\\n",
      "0  0.016008  0.032017  0.016008   0.0  0.016008  0.00000  0.000000   \n",
      "1  0.019566  0.019566  0.019566   0.0  0.019566  0.00000  0.053013   \n",
      "2  0.000000  0.000000  0.000000   0.0  0.000000  0.11928  0.000000   \n",
      "\n",
      "   scientists      this    fields   science   courses  important      most  \n",
      "0     0.00000  0.000000  0.043375  0.032017  0.000000   0.043375  0.043375  \n",
      "1     0.00000  0.053013  0.000000  0.019566  0.053013   0.000000  0.000000  \n",
      "2     0.11928  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19833/1472978829.py:30: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_tf[w][i] += 1 / len(words)\n",
      "/tmp/ipykernel_19833/1472978829.py:56: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Construct a small corpus\n",
    "corpus = [\n",
    "    \"data science is one of the most important fields of science\",\n",
    "    \"this is one of the best data science courses\",\n",
    "    \"data scientists analyze data\"\n",
    "]\n",
    "\n",
    "\n",
    "# Create a set of all unique words in the corpus\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split()\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number of words in the corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "n_docs = len(corpus)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Create a DataFrame to store TF values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
    "\n",
    "# Calculate TF for each word in each document\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split()\n",
    "    for w in words:\n",
    "        # Calculate TF as the number of occurrences of the word divided by the total number of words in the document\n",
    "        df_tf[w][i] += 1 / len(words)\n",
    "\n",
    "print(\"Term Frequency (TF):\")\n",
    "print(df_tf)\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "# Calculate IDF for each word\n",
    "for w in words_set:\n",
    "    k = 0  # Number of documents containing the word\n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "    # Calculate IDF using the log of the number of documents divided by the number of documents containing the word\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "    print(f\"{w:>15}: {idf[w]:>10}\")\n",
    "\n",
    "# Compute TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "# Calculate TF-IDF for each word in each document\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of word embedding: 50\n",
      "Words most similar to 'king': [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721), ('uncle', 0.7627150416374207), ('kingdom', 0.7542160749435425), ('throne', 0.7539913654327393), ('brother', 0.7492411136627197), ('ruler', 0.7434253692626953)]\n",
      "Similarity between 'king' and 'queen': 0.7839043\n",
      "Vector representation of 'king - man + woman': [('ruler', 0.7395833134651184)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the pre-trained Word2Vec model from Google (takes some time)\n",
    "# This downloads a large file, so be patient.\n",
    "word2vec_model_path = api.load(\"glove-wiki-gigaword-50\", return_path=True)\n",
    "\n",
    "# Load the Word2Vec model using gensim\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False)\n",
    "\n",
    "# Example: Get the word embedding for the word \"king\"\n",
    "word_embedding = word2vec_model[\"king\"]\n",
    "\n",
    "# Print the dimensionality of the word embedding\n",
    "print(\"Dimensionality of word embedding:\", len(word_embedding))\n",
    "\n",
    "# Example: Get the most similar words to \"king\"\n",
    "similar_words = word2vec_model.most_similar(\"king\")\n",
    "print(\"Words most similar to 'king':\", similar_words)\n",
    "\n",
    "# Example: Calculate the similarity between two words\n",
    "similarity_score = word2vec_model.similarity(\"king\", \"queen\")\n",
    "print(\"Similarity between 'king' and 'queen':\", similarity_score)\n",
    "\n",
    "# Example: Calculate the vector representing the combination of words \"king\" and \"man\" minus \"woman\"\n",
    "result_vector = word2vec_model.most_similar(positive=[\"king\", \"man\"], negative=[\"woman\"], topn=1)\n",
    "print(\"Vector representation of 'king - man + woman':\", result_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GLove [ref 🔗](https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db)  \n",
    "\n",
    "Global Vectors for Word Representation, or GloVe for short, is an unsupervised learning algorithm that generates vector representations, or embeddings, of words. Researchers Richard Socher, Christopher D. Manning, and Jeffrey Pennington first presented it in 2014. By using the statistical co-occurrence data of words in a given corpus, GloVe is intended to capture the semantic relationships between words.\n",
    "\n",
    "The fundamental concept underlying GloVe is the representation of words as vectors in a continuous vector space, where the angle and direction of the vectors correspond to the semantic connections between the appropriate words. To do this, GloVe builds a co-occurrence matrix using word pairs and then optimizes the word vectors to minimize the difference between the pointwise mutual information of the corresponding words and the dot product of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC1\n",
      "the [-6.5276e-01  2.3873e-01 -2.3325e-01  1.8608e-01  3.7674e-01 -5.4116e-02\n",
      " -1.9189e-01  2.2731e-01 -9.2528e-02  1.8388e+00 -1.8715e-01 -2.2237e-01\n",
      "  3.1873e-01  1.1472e-01  3.8304e-01  2.0092e-01 -2.7932e-01  2.3462e+00\n",
      " -3.9846e-01 -1.9525e-01 -2.5649e-01  2.5508e-01  9.4618e-02 -4.1082e-01\n",
      " -3.4191e-01 -1.9499e-01  1.7814e-01  5.3463e-03 -4.7565e-01  2.8022e-01\n",
      " -2.1920e-01  6.0433e-01  2.9309e-01 -2.4232e-01  5.2700e-01  3.9024e-01\n",
      " -5.6955e-01  3.7620e-01 -2.3126e-01 -2.9921e-01  4.5643e-02  1.4555e-01\n",
      "  1.4231e-01  1.0587e-01  4.1210e-01 -2.5261e-01 -3.2090e-02 -5.2830e-01\n",
      " -2.6925e-02  2.6227e-01  1.6375e-01  9.9259e-02  3.1664e-01 -1.1040e-01\n",
      "  2.5732e-01 -4.0720e-01 -6.9903e-02 -1.3189e-01 -5.5753e-01 -1.4815e-01\n",
      " -3.3673e-01 -3.6122e-01  2.1905e-02  6.8589e-01 -8.0151e-02 -1.2327e-01\n",
      " -5.0595e-02 -1.3694e-01  2.7306e-01 -1.4885e-01  4.5150e-03  1.6709e-01\n",
      "  8.8148e-02 -1.7480e-01 -1.2441e-01  5.6394e-02  1.4228e-01  3.6865e-01\n",
      "  5.9486e-02 -1.9471e-01 -2.5575e-01  2.2853e-01 -8.2876e-03  2.0285e-02\n",
      "  4.2212e-01  1.3516e-01  7.2635e-01  3.8426e-01  1.9571e-01 -1.8199e-01\n",
      "  3.6326e-01 -5.9009e-01  1.2660e-01 -2.7157e-01 -4.0160e-01 -6.2389e-01\n",
      "  7.8656e-01  2.8767e-01  5.0652e-01  9.7881e-02 -1.4708e-02  2.3946e-01\n",
      " -1.8723e-01  4.1445e-01  1.6650e-01 -1.8899e+00  3.5874e-01 -2.9300e-02\n",
      " -7.1390e-02  7.2308e-02  6.5537e-02 -1.5800e-01  8.0230e-02 -3.0899e-02\n",
      "  1.9015e-01 -1.4525e-02  9.2995e-03 -4.5801e-01 -5.1586e-02 -4.5419e-02\n",
      "  3.9683e-01 -2.5542e-01  2.4292e-01 -3.0690e-01 -2.4360e-01 -2.7307e-01\n",
      "  1.4416e-01 -9.6950e-03 -5.7948e-01 -2.2177e-01  9.3387e-02 -3.0545e-01\n",
      " -7.1946e-02  2.3493e-01  7.2558e-01  5.4868e-02 -2.7755e-01  1.6177e-01\n",
      " -3.9068e-01 -2.3836e-02 -3.8402e-01  2.9663e-02 -7.9789e-01 -2.6405e-01\n",
      "  5.8656e-02  1.4091e-01  3.0928e-01  1.4004e-01  1.3282e-02  2.1340e-01\n",
      " -3.4682e-01  3.5323e-01  2.9809e-01  9.6521e-02 -2.2186e-02 -1.5502e-01\n",
      " -7.5963e-02  1.0163e+00 -2.0319e-01 -4.8089e-01  1.8526e-03  3.0310e-01\n",
      " -3.0456e-01 -2.9545e-01 -1.1845e-02 -1.5618e-01  2.7693e-01  3.1751e-01\n",
      "  3.6650e-01 -3.9038e-01  4.2004e-01 -3.2852e-01 -6.7994e-02 -1.4249e-01\n",
      " -1.3815e-01 -1.0507e-01 -1.7883e-01 -3.8480e-01  3.2119e-01 -1.8403e-01\n",
      " -5.3652e-02 -2.0844e-02 -7.6724e-01 -1.1748e-01 -7.7480e-02  4.5177e-01\n",
      "  9.6727e-02 -2.0082e-01 -1.9445e-01 -9.7588e-03  3.0302e-01 -3.3607e-01\n",
      "  2.3825e-01 -3.6793e-02 -1.5106e-01 -8.4877e-02 -1.7448e-01 -1.5700e-01\n",
      " -6.5094e-01  1.4706e-02  4.4220e-01  8.0152e-03 -3.4658e-01  2.3566e-01\n",
      "  1.5767e-01 -1.6591e-01 -2.4740e-01 -3.3408e-01 -1.9722e-01  2.2875e-02\n",
      " -1.4465e-01 -2.1910e-01 -9.2784e-02 -4.1141e-01  2.7569e-01 -5.2462e-02\n",
      " -1.1711e-01  2.3524e-01  1.3615e-01  2.0582e-01  3.3483e-04 -3.5967e-02\n",
      " -4.2995e-01 -3.1407e-01  2.7588e-01 -2.0524e-01 -1.7360e-01 -8.1571e-02\n",
      "  3.2323e-01  2.3840e-01  1.2790e-01 -4.0717e-01  5.8714e-01  3.2730e-01\n",
      "  1.7860e-02  1.1729e-01 -4.7061e-02 -7.8453e-02  1.4081e-01 -1.5994e-01\n",
      " -1.4909e-01  7.1805e-02 -3.7698e-02  4.4769e-01  1.6152e-01 -8.2816e-02\n",
      " -1.1207e-01 -3.8554e-02 -1.0233e-01  6.8006e-02  1.6570e-01 -1.8337e-01\n",
      "  3.8589e-01  5.7057e-03 -1.4620e-01  1.2894e+00  3.4140e-01  1.6715e-01\n",
      "  4.2835e-01 -1.0456e-01 -3.0015e-01 -2.4827e-02  2.5433e-01  1.8072e-01\n",
      "  2.6348e-01 -6.7580e-02  1.7232e-01 -7.1153e-02  1.7512e-01 -2.7813e-02\n",
      "  6.4046e-01 -2.7157e-01 -9.4352e-02 -2.2393e-01  8.9257e-03 -3.7729e-01\n",
      " -4.0433e-02  1.1951e-01 -2.2956e-01  3.5904e-01  4.1923e-02  5.3552e-02\n",
      " -1.4838e-01  5.5642e-01 -1.1883e-01  1.1237e-01  3.3574e-04 -1.5964e-01\n",
      "  1.9147e-01  1.1882e-01 -2.8518e-01 -7.7427e-02  2.2063e-01 -2.9288e-01\n",
      " -5.9751e-01 -1.6613e-02 -2.1909e-02 -4.2636e-01  4.8578e-01 -2.8969e-01]\n",
      "king [-6.0644e-01 -5.1205e-01  6.4921e-03 -2.9194e-01 -5.6515e-01 -1.1523e-01\n",
      "  7.7274e-02  3.3561e-01  1.1593e-01  2.3516e+00  5.1773e-02 -5.4229e-01\n",
      " -5.7972e-01  1.3220e-01  2.8430e-01 -7.9592e-02 -2.6762e-01  1.8301e-01\n",
      " -4.1264e-01  2.0459e-01  1.4436e-01 -1.8714e-01 -3.1393e-01  1.7821e-01\n",
      " -1.0997e-01 -2.5584e-01 -1.1149e-01  9.6212e-02 -1.6168e-01  4.0055e-01\n",
      " -2.6115e-01  5.3777e-01 -5.2382e-01  2.7637e-01  7.2191e-01  6.0405e-02\n",
      " -1.7922e-01  1.8020e-01 -1.4381e-01 -1.4795e-01 -8.1394e-02  5.8282e-02\n",
      "  2.2964e-02 -2.6374e-01  1.0704e-01 -4.5425e-01 -1.9964e-01  3.7720e-01\n",
      " -9.7784e-02 -3.1999e-01 -7.8509e-02  6.1502e-01  7.1643e-02 -3.0930e-02\n",
      "  2.1508e-01  2.5280e-01 -3.1643e-01  6.6698e-01  1.9813e-02 -3.2311e-01\n",
      "  2.9266e-02 -4.1403e-02  2.8346e-01 -7.9143e-01  1.3327e-01  7.7231e-02\n",
      " -1.8724e-01 -3.3146e-01 -2.0797e-01 -6.9326e-01 -2.3412e-01 -6.8752e-02\n",
      "  3.8252e-02 -3.2459e-01 -8.3609e-03  1.2945e-01 -2.8316e-01 -5.7546e-01\n",
      "  2.4336e-01  5.6433e-01 -7.1285e-01 -5.4738e-03 -2.3305e-01 -7.1578e-02\n",
      "  4.8301e-01 -3.4312e-01  2.7365e-01 -1.1771e+00 -6.5800e-01 -1.9009e-01\n",
      "  7.4287e-03  3.2977e-01 -1.6647e-01  2.6851e-01  1.1811e-01 -6.2440e-02\n",
      " -4.9987e-02  7.1011e-04 -5.6201e-02 -2.6696e-01  3.1351e-01  4.3955e-01\n",
      " -8.8727e-02 -1.2315e-01  1.8855e-01 -1.0834e+00 -3.3041e-01  5.7325e-01\n",
      " -3.9947e-01  1.4852e-02 -3.6787e-01  3.7842e-01 -2.8962e-01 -7.0543e-02\n",
      " -5.8699e-02  5.3076e-01 -1.2736e-01 -3.5724e-01 -1.5007e-01  1.3823e-02\n",
      " -1.9497e-01 -3.7189e-01  2.6255e-01 -7.6826e-02  8.4217e-02 -5.3640e-01\n",
      "  1.7393e-01 -1.4698e-01 -1.1068e-01  1.7709e-01 -3.9556e-01  1.0433e-01\n",
      "  9.2675e-03 -1.2282e-01 -3.9842e-01 -2.7758e-01 -6.9369e-01  7.0128e-02\n",
      "  8.2794e-02  4.8342e-02 -2.7038e+00 -1.6812e-01  3.1413e-01  2.4313e-02\n",
      " -3.6423e-02  1.9292e-01  4.4872e-01 -4.5427e-01 -3.7271e-01 -9.9532e-01\n",
      " -1.3411e-01 -6.0312e-01  1.6642e-01 -2.4611e-02  6.6891e-01  6.3476e-02\n",
      " -1.1327e+00 -3.3786e-01 -1.2576e-02  3.5344e-01  2.6643e-01 -1.9404e-01\n",
      " -1.9516e-01  6.3670e-01  2.1373e-01 -2.8936e-01 -6.8847e-02 -1.9738e-01\n",
      " -3.5305e-01  1.0219e-01  1.1744e-01  3.7159e-02  4.1041e-01 -1.3766e-02\n",
      " -1.0325e-02  1.0461e-02  3.0697e-02 -3.3016e-01  2.4668e-01 -2.6058e-01\n",
      "  2.8665e-01 -7.8507e-02  6.8945e-03  1.0980e-01 -6.4179e-01  2.4617e-03\n",
      " -2.4693e-01 -1.1188e-02  3.0838e-01  4.5557e-01 -6.2189e-01  1.4873e-01\n",
      "  3.5440e-01  2.8642e-01 -2.4211e-01 -1.2404e-01  2.3326e-01  1.9555e-01\n",
      " -1.2425e-02  1.9920e-01 -1.7935e-01  5.2031e-01 -4.3666e-01  8.6211e-02\n",
      "  1.7282e-01  6.5266e-02  2.8701e-01  6.0238e-01  3.1843e-01 -4.7646e-01\n",
      " -2.1181e-02 -2.7726e-01  4.0253e-01  3.9968e-01  1.8580e-02 -6.2663e-01\n",
      "  3.4149e-01  4.4687e-01 -4.6135e-01  4.4174e-01 -5.7541e-02 -1.9038e-02\n",
      " -2.2626e-01  5.8452e-02 -4.6681e-02 -5.3295e-03 -1.8257e-03  4.8565e-01\n",
      " -4.6144e-01 -4.5877e-01 -1.5891e-01  1.3037e-01 -2.9183e-01  6.9206e-02\n",
      " -4.9825e-02  5.5077e-01  1.4730e-01 -1.9255e-01 -2.3916e-01 -1.9319e-01\n",
      "  1.5643e-01  3.3491e-01 -3.1913e-01  2.0674e-01  6.4556e-02 -2.3195e-01\n",
      "  1.2657e-01 -2.5131e-03  1.1079e-01  3.0436e-01  6.9529e-02  1.1027e-01\n",
      "  2.6285e-01 -2.3103e-01 -2.8933e-01 -5.0675e-02 -8.9796e-02  2.5816e-01\n",
      " -8.0917e-02  3.3160e-01 -3.5930e-01  2.8336e-01  1.4145e-01  2.9012e-01\n",
      "  1.5677e-01  1.3225e-01 -5.0090e-01  2.2110e-01  6.9609e-01 -9.6917e-02\n",
      " -2.4966e-02 -2.9391e-01 -3.1240e-01 -3.8031e-01 -2.0604e-01  1.5959e-01\n",
      " -5.6155e-01  2.9170e-01 -5.0459e-01  6.5684e-02  5.8594e-01  1.3003e-02\n",
      "  6.5874e-01 -4.7811e-01  2.8794e-01  3.5918e-01  4.3347e-01 -4.2480e-01\n",
      "  3.5892e-01 -6.0925e-01 -7.1236e-01  2.9490e-01 -2.1479e-01  2.5658e-01\n",
      " -1.9358e-01  1.1057e+00  2.2862e-01  2.1859e-01 -1.9044e-01 -1.0253e-01]\n",
      "is [-6.0053e-01  1.8838e-01 -4.0993e-01  3.2250e-01  7.0322e-02  1.6176e-01\n",
      "  3.6945e-01 -4.1277e-01 -3.9160e-02  2.2285e+00  2.5742e-01 -2.3096e-01\n",
      " -7.2397e-02 -5.7500e-02 -3.1920e-01 -2.7190e-01 -2.6441e-01  1.3977e+00\n",
      " -1.4000e-01 -3.3425e-01 -5.5235e-01 -3.3982e-01 -1.8204e-01 -7.0568e-01\n",
      " -2.7514e-01  1.3087e-01  1.9301e-01  5.7467e-02 -2.7035e-02 -8.3781e-01\n",
      " -1.3592e-01 -3.5228e-01 -3.0606e-02 -2.2015e-01 -3.1897e-01 -1.5749e-02\n",
      "  1.2286e-01 -2.4969e-01 -2.3830e-01 -1.6646e-01  2.8786e-01 -6.8925e-02\n",
      "  1.6608e-01  6.2562e-02  2.4973e-01 -4.9537e-02 -8.2922e-02 -1.3069e-01\n",
      " -1.9901e-01  2.6164e-02 -4.0458e-01  5.3881e-01  6.3147e-02 -3.9753e-02\n",
      "  1.6650e-01  7.2560e-02  9.7908e-02 -7.0421e-02  1.9828e-01  1.2767e-01\n",
      "  1.1819e-01 -2.6825e-01  7.8166e-02 -1.1828e-01 -9.8565e-02 -3.1605e-01\n",
      " -6.5541e-02  7.8084e-02  2.0808e-01  1.0000e-01  1.8866e-01  8.2198e-02\n",
      " -5.3661e-02 -2.5744e-01  2.8765e-01  1.9885e-01  2.9937e-02 -1.5602e-01\n",
      " -6.1491e-01  4.7391e-01  1.1359e-01 -1.1528e-01 -2.3069e-02  3.9535e-01\n",
      "  2.5414e-01 -4.6473e-01 -2.4887e-01  5.2441e-01 -1.2795e-01 -3.5715e-01\n",
      " -1.0261e-01  2.9131e-01 -3.7341e-01  2.6126e-01 -3.0449e-02 -1.7962e-01\n",
      "  1.7215e-01 -4.8918e-01 -6.8684e-02 -7.4713e-02  2.5949e-01  6.7295e-02\n",
      " -1.5749e-01  3.8877e-02  3.9118e-01 -1.6235e+00 -3.4327e-01 -4.4498e-01\n",
      "  1.4854e-01 -2.1732e-02  1.9457e-01 -1.3311e-01  1.6695e-01 -2.1000e-01\n",
      " -2.7190e-01 -1.6934e-01 -2.1079e-01 -1.3167e-01 -1.3745e-01  2.9225e-02\n",
      "  1.9389e-03  2.8961e-01 -1.9132e-01 -3.7929e-01 -6.5660e-02  1.4801e-01\n",
      "  9.3407e-02 -4.0992e-01 -5.8972e-02  2.5662e-02  1.0114e-01  4.2247e-02\n",
      " -2.6918e-01  9.7664e-02 -7.9141e-02 -4.9951e-02 -3.8281e-01 -2.5272e-01\n",
      "  1.6170e-01 -3.8093e-02 -1.0397e+00  3.9195e-01  4.8628e-01  1.9720e-01\n",
      "  7.0788e-02 -5.6636e-01 -7.9355e-02 -1.6658e-01  4.9513e-01  9.4517e-02\n",
      "  3.3129e-02  5.2600e-01 -4.8114e-02  1.0961e-01 -2.0926e-01  5.3785e-01\n",
      " -1.9596e-01 -8.1739e-03 -1.5444e-01 -8.0581e-01 -1.0557e-01 -3.3322e-01\n",
      "  4.6406e-01  9.9518e-02  2.1007e-02 -1.3025e-01  9.2547e-02 -1.7638e-02\n",
      " -1.4717e-01 -1.2243e-01 -6.3543e-02  9.0688e-02  4.7598e-02  1.1890e-01\n",
      " -1.1482e-01 -4.4486e-01 -1.2677e-01 -8.1982e-02  1.4991e-02  6.5389e-01\n",
      "  1.3691e-01 -9.8414e-03 -1.0417e-01  3.8314e-02  9.8986e-04 -5.5078e-01\n",
      "  1.2765e-01 -7.2308e-02  4.9886e-03  2.1618e-01  1.1043e-01  9.3522e-02\n",
      " -5.5075e-02  3.1022e-01 -2.4428e-01 -2.8605e-01  1.9502e-01  9.3397e-02\n",
      " -2.2314e-01 -3.7717e-02  7.8812e-02 -1.3824e-01  1.9336e-02  5.7978e-02\n",
      " -2.4335e-01  2.4662e-01 -3.2615e-01 -6.5513e-02 -2.8090e-01 -8.1939e-03\n",
      "  8.4427e-02  7.6577e-02  9.4961e-02 -5.1576e-01  2.7032e-01  4.0810e-01\n",
      "  5.9457e-01  1.2561e-01 -9.8435e-02 -2.7894e-01  6.2252e-02  1.7876e-01\n",
      "  5.0321e-02  1.2027e-03  1.3166e-01 -9.9114e-02 -4.2881e-01 -7.4613e-02\n",
      " -1.6439e-01  2.6544e-01  3.6246e-01 -4.1890e-01 -2.4099e-01  3.7963e-02\n",
      "  2.6231e-01 -2.2149e-01 -2.2201e-01 -2.7069e-01 -3.1194e-02  3.0734e-01\n",
      "  3.7519e-01  7.3105e-02 -9.4563e-02  2.5243e-01  1.8636e-01  8.4880e-02\n",
      " -5.3694e-01 -3.4793e-01  1.8903e-01  3.0854e-01 -3.1863e-01  1.5979e-01\n",
      " -3.7042e-02 -1.2272e-01 -4.6573e-02  4.7312e-02  4.5770e-01  1.3056e-01\n",
      " -9.8451e-02  3.8605e-01  1.9225e-01  2.8049e-01  1.3640e-01 -1.5697e-01\n",
      " -8.0680e-02  1.0271e-01 -8.8975e-02  4.8581e-02  2.7049e-01  4.9596e-01\n",
      " -5.7315e-02 -1.6473e-01 -5.2998e-01 -7.2100e-01 -4.1816e-01  8.3745e-02\n",
      "  2.9744e-01 -9.6166e-02 -7.4129e-02  6.2675e-01  3.8443e-01  3.2924e-01\n",
      "  7.5049e-02  1.2777e-01 -2.4787e-02  1.1350e-01  8.8798e-02 -3.4373e-01\n",
      "  3.8127e-01 -1.1508e-01 -3.0752e-01  1.8114e-01  2.1235e-02  1.2783e-01\n",
      "  3.5292e-01  5.8156e-01  3.6282e-01 -2.7799e-01  3.1229e-01 -2.8331e-01]\n",
      "great [-6.0053e-01  1.8838e-01 -4.0993e-01  3.2250e-01  7.0322e-02  1.6176e-01\n",
      "  3.6945e-01 -4.1277e-01 -3.9160e-02  2.2285e+00  2.5742e-01 -2.3096e-01\n",
      " -7.2397e-02 -5.7500e-02 -3.1920e-01 -2.7190e-01 -2.6441e-01  1.3977e+00\n",
      " -1.4000e-01 -3.3425e-01 -5.5235e-01 -3.3982e-01 -1.8204e-01 -7.0568e-01\n",
      " -2.7514e-01  1.3087e-01  1.9301e-01  5.7467e-02 -2.7035e-02 -8.3781e-01\n",
      " -1.3592e-01 -3.5228e-01 -3.0606e-02 -2.2015e-01 -3.1897e-01 -1.5749e-02\n",
      "  1.2286e-01 -2.4969e-01 -2.3830e-01 -1.6646e-01  2.8786e-01 -6.8925e-02\n",
      "  1.6608e-01  6.2562e-02  2.4973e-01 -4.9537e-02 -8.2922e-02 -1.3069e-01\n",
      " -1.9901e-01  2.6164e-02 -4.0458e-01  5.3881e-01  6.3147e-02 -3.9753e-02\n",
      "  1.6650e-01  7.2560e-02  9.7908e-02 -7.0421e-02  1.9828e-01  1.2767e-01\n",
      "  1.1819e-01 -2.6825e-01  7.8166e-02 -1.1828e-01 -9.8565e-02 -3.1605e-01\n",
      " -6.5541e-02  7.8084e-02  2.0808e-01  1.0000e-01  1.8866e-01  8.2198e-02\n",
      " -5.3661e-02 -2.5744e-01  2.8765e-01  1.9885e-01  2.9937e-02 -1.5602e-01\n",
      " -6.1491e-01  4.7391e-01  1.1359e-01 -1.1528e-01 -2.3069e-02  3.9535e-01\n",
      "  2.5414e-01 -4.6473e-01 -2.4887e-01  5.2441e-01 -1.2795e-01 -3.5715e-01\n",
      " -1.0261e-01  2.9131e-01 -3.7341e-01  2.6126e-01 -3.0449e-02 -1.7962e-01\n",
      "  1.7215e-01 -4.8918e-01 -6.8684e-02 -7.4713e-02  2.5949e-01  6.7295e-02\n",
      " -1.5749e-01  3.8877e-02  3.9118e-01 -1.6235e+00 -3.4327e-01 -4.4498e-01\n",
      "  1.4854e-01 -2.1732e-02  1.9457e-01 -1.3311e-01  1.6695e-01 -2.1000e-01\n",
      " -2.7190e-01 -1.6934e-01 -2.1079e-01 -1.3167e-01 -1.3745e-01  2.9225e-02\n",
      "  1.9389e-03  2.8961e-01 -1.9132e-01 -3.7929e-01 -6.5660e-02  1.4801e-01\n",
      "  9.3407e-02 -4.0992e-01 -5.8972e-02  2.5662e-02  1.0114e-01  4.2247e-02\n",
      " -2.6918e-01  9.7664e-02 -7.9141e-02 -4.9951e-02 -3.8281e-01 -2.5272e-01\n",
      "  1.6170e-01 -3.8093e-02 -1.0397e+00  3.9195e-01  4.8628e-01  1.9720e-01\n",
      "  7.0788e-02 -5.6636e-01 -7.9355e-02 -1.6658e-01  4.9513e-01  9.4517e-02\n",
      "  3.3129e-02  5.2600e-01 -4.8114e-02  1.0961e-01 -2.0926e-01  5.3785e-01\n",
      " -1.9596e-01 -8.1739e-03 -1.5444e-01 -8.0581e-01 -1.0557e-01 -3.3322e-01\n",
      "  4.6406e-01  9.9518e-02  2.1007e-02 -1.3025e-01  9.2547e-02 -1.7638e-02\n",
      " -1.4717e-01 -1.2243e-01 -6.3543e-02  9.0688e-02  4.7598e-02  1.1890e-01\n",
      " -1.1482e-01 -4.4486e-01 -1.2677e-01 -8.1982e-02  1.4991e-02  6.5389e-01\n",
      "  1.3691e-01 -9.8414e-03 -1.0417e-01  3.8314e-02  9.8986e-04 -5.5078e-01\n",
      "  1.2765e-01 -7.2308e-02  4.9886e-03  2.1618e-01  1.1043e-01  9.3522e-02\n",
      " -5.5075e-02  3.1022e-01 -2.4428e-01 -2.8605e-01  1.9502e-01  9.3397e-02\n",
      " -2.2314e-01 -3.7717e-02  7.8812e-02 -1.3824e-01  1.9336e-02  5.7978e-02\n",
      " -2.4335e-01  2.4662e-01 -3.2615e-01 -6.5513e-02 -2.8090e-01 -8.1939e-03\n",
      "  8.4427e-02  7.6577e-02  9.4961e-02 -5.1576e-01  2.7032e-01  4.0810e-01\n",
      "  5.9457e-01  1.2561e-01 -9.8435e-02 -2.7894e-01  6.2252e-02  1.7876e-01\n",
      "  5.0321e-02  1.2027e-03  1.3166e-01 -9.9114e-02 -4.2881e-01 -7.4613e-02\n",
      " -1.6439e-01  2.6544e-01  3.6246e-01 -4.1890e-01 -2.4099e-01  3.7963e-02\n",
      "  2.6231e-01 -2.2149e-01 -2.2201e-01 -2.7069e-01 -3.1194e-02  3.0734e-01\n",
      "  3.7519e-01  7.3105e-02 -9.4563e-02  2.5243e-01  1.8636e-01  8.4880e-02\n",
      " -5.3694e-01 -3.4793e-01  1.8903e-01  3.0854e-01 -3.1863e-01  1.5979e-01\n",
      " -3.7042e-02 -1.2272e-01 -4.6573e-02  4.7312e-02  4.5770e-01  1.3056e-01\n",
      " -9.8451e-02  3.8605e-01  1.9225e-01  2.8049e-01  1.3640e-01 -1.5697e-01\n",
      " -8.0680e-02  1.0271e-01 -8.8975e-02  4.8581e-02  2.7049e-01  4.9596e-01\n",
      " -5.7315e-02 -1.6473e-01 -5.2998e-01 -7.2100e-01 -4.1816e-01  8.3745e-02\n",
      "  2.9744e-01 -9.6166e-02 -7.4129e-02  6.2675e-01  3.8443e-01  3.2924e-01\n",
      "  7.5049e-02  1.2777e-01 -2.4787e-02  1.1350e-01  8.8798e-02 -3.4373e-01\n",
      "  3.8127e-01 -1.1508e-01 -3.0752e-01  1.8114e-01  2.1235e-02  1.2783e-01\n",
      "  3.5292e-01  5.8156e-01  3.6282e-01 -2.7799e-01  3.1229e-01 -2.8331e-01]\n",
      "DOC2\n",
      "man [-0.60482    0.37399    0.28446   -0.016269  -0.11893   -0.19706\n",
      " -0.35118   -0.065699  -0.21866    3.2252     0.26044   -0.088566\n",
      " -0.038691   0.37517   -0.98321   -0.45542   -0.15896   -0.30224\n",
      " -0.25036   -0.29216   -0.24617    0.11819   -0.078674   0.093279\n",
      "  0.12809   -0.32756   -0.97027   -0.044002   0.35486    0.033635\n",
      " -0.36844   -0.11504   -0.47169    0.098168   0.142     -0.18667\n",
      "  0.19343   -0.14393    0.14944    0.23605    0.066285  -0.38853\n",
      "  0.53206   -0.37733    0.24188   -0.18147   -0.44303    0.23806\n",
      "  0.2259     0.40275   -0.37308   -0.17719   -0.18686    0.5844\n",
      " -0.67599    0.37659   -0.43857   -0.15042    0.27707    0.41077\n",
      " -0.1408     0.022517   0.24255    0.24385    0.46896    0.39303\n",
      " -0.07902   -0.01428    0.1692     0.14497   -0.17556    0.12782\n",
      " -0.36938   -0.094289   0.25163   -0.69674    0.43107   -0.17208\n",
      "  0.081579  -0.064771   0.45584    0.35017    0.29294   -0.30455\n",
      "  0.11419    0.21348    0.29734   -1.2992    -0.56742    0.17874\n",
      "  0.012806   0.23741   -0.15764   -0.57683   -0.17482   -0.6367\n",
      "  0.049154   0.12277   -0.1048    -0.054331  -0.3931     0.90383\n",
      "  0.41789   -0.073492  -0.043529  -0.29609   -0.46518    0.2535\n",
      "  0.43634   -0.10809   -0.069687  -0.17759   -0.25754   -0.028724\n",
      " -0.23559    0.25251    0.59274    0.044534  -0.43289   -0.0043122\n",
      " -0.066697  -0.16242    1.0618    -0.8897     0.15056   -0.25966\n",
      " -0.38773   -0.38122   -0.25842   -0.040987  -0.38859    0.27229\n",
      " -0.058356   0.033234   0.30679   -0.52575   -0.071837   0.20053\n",
      " -0.082414   0.31865   -1.8504     0.088368   0.22011    0.36031\n",
      "  0.47597    0.5198    -0.593     -0.24768    0.061518  -0.60215\n",
      " -0.57487    0.5612     0.39073    0.15437   -0.11999    0.30493\n",
      " -0.36427    0.32849   -0.087544   0.041692   0.096959  -0.38704\n",
      "  0.1713     0.6312     0.17268    0.23724   -0.36598   -0.1257\n",
      " -0.021616   0.23721    0.42932    0.28287    0.20133    0.0041654\n",
      " -0.12174   -0.25234   -0.2387    -0.24326    0.23537   -0.52293\n",
      "  0.20751    0.31524   -0.33251    0.090908  -0.1953     0.44603\n",
      "  0.38114    0.10406    0.24136   -0.1105     0.39616    0.35996\n",
      " -0.13601    0.8062     0.14567    0.055947   0.14811   -0.25804\n",
      " -0.22502   -0.72999   -0.15063    0.18176    0.2145     0.024322\n",
      "  0.2816     0.32362   -0.11707    0.2983     0.078185  -0.47034\n",
      "  0.26954    0.41545   -0.14821    0.72835    0.17427   -1.2983\n",
      " -0.2734     0.054035   0.49905   -0.1452     0.20773    0.53079\n",
      " -0.088859   0.0041812 -0.41446    0.15825    0.54879    0.60234\n",
      " -0.054374   0.17053   -0.63028   -0.035762  -0.16963   -0.58431\n",
      " -0.094292  -0.35639    0.21692    0.57807    0.16884    0.0084035\n",
      " -0.16317   -0.02344    0.19697    0.033108   0.10194    0.10325\n",
      "  0.58115    0.20838    0.40825   -0.31832   -0.67854    0.56479\n",
      "  0.62659    0.17756    0.11136   -0.070346  -0.36849   -0.029168\n",
      "  0.084723  -0.0072435  0.1289     0.24038   -0.30857   -0.12679\n",
      " -0.40641   -0.38458    0.16146    0.35935   -0.71341    0.33389\n",
      "  0.71917    0.049543  -0.080449  -0.18128    0.06475    0.47245\n",
      " -0.26618    0.33566    0.28451    0.079369   0.12202    0.28023\n",
      " -0.46695   -0.46264    0.34407   -0.082376   0.26314   -0.28303\n",
      " -0.48122   -0.14167   -0.28421   -0.20905   -0.2405    -0.15659\n",
      " -0.38115   -0.34089   -0.67244   -0.13601    0.30378   -0.084864 ]\n",
      "is [-6.0053e-01  1.8838e-01 -4.0993e-01  3.2250e-01  7.0322e-02  1.6176e-01\n",
      "  3.6945e-01 -4.1277e-01 -3.9160e-02  2.2285e+00  2.5742e-01 -2.3096e-01\n",
      " -7.2397e-02 -5.7500e-02 -3.1920e-01 -2.7190e-01 -2.6441e-01  1.3977e+00\n",
      " -1.4000e-01 -3.3425e-01 -5.5235e-01 -3.3982e-01 -1.8204e-01 -7.0568e-01\n",
      " -2.7514e-01  1.3087e-01  1.9301e-01  5.7467e-02 -2.7035e-02 -8.3781e-01\n",
      " -1.3592e-01 -3.5228e-01 -3.0606e-02 -2.2015e-01 -3.1897e-01 -1.5749e-02\n",
      "  1.2286e-01 -2.4969e-01 -2.3830e-01 -1.6646e-01  2.8786e-01 -6.8925e-02\n",
      "  1.6608e-01  6.2562e-02  2.4973e-01 -4.9537e-02 -8.2922e-02 -1.3069e-01\n",
      " -1.9901e-01  2.6164e-02 -4.0458e-01  5.3881e-01  6.3147e-02 -3.9753e-02\n",
      "  1.6650e-01  7.2560e-02  9.7908e-02 -7.0421e-02  1.9828e-01  1.2767e-01\n",
      "  1.1819e-01 -2.6825e-01  7.8166e-02 -1.1828e-01 -9.8565e-02 -3.1605e-01\n",
      " -6.5541e-02  7.8084e-02  2.0808e-01  1.0000e-01  1.8866e-01  8.2198e-02\n",
      " -5.3661e-02 -2.5744e-01  2.8765e-01  1.9885e-01  2.9937e-02 -1.5602e-01\n",
      " -6.1491e-01  4.7391e-01  1.1359e-01 -1.1528e-01 -2.3069e-02  3.9535e-01\n",
      "  2.5414e-01 -4.6473e-01 -2.4887e-01  5.2441e-01 -1.2795e-01 -3.5715e-01\n",
      " -1.0261e-01  2.9131e-01 -3.7341e-01  2.6126e-01 -3.0449e-02 -1.7962e-01\n",
      "  1.7215e-01 -4.8918e-01 -6.8684e-02 -7.4713e-02  2.5949e-01  6.7295e-02\n",
      " -1.5749e-01  3.8877e-02  3.9118e-01 -1.6235e+00 -3.4327e-01 -4.4498e-01\n",
      "  1.4854e-01 -2.1732e-02  1.9457e-01 -1.3311e-01  1.6695e-01 -2.1000e-01\n",
      " -2.7190e-01 -1.6934e-01 -2.1079e-01 -1.3167e-01 -1.3745e-01  2.9225e-02\n",
      "  1.9389e-03  2.8961e-01 -1.9132e-01 -3.7929e-01 -6.5660e-02  1.4801e-01\n",
      "  9.3407e-02 -4.0992e-01 -5.8972e-02  2.5662e-02  1.0114e-01  4.2247e-02\n",
      " -2.6918e-01  9.7664e-02 -7.9141e-02 -4.9951e-02 -3.8281e-01 -2.5272e-01\n",
      "  1.6170e-01 -3.8093e-02 -1.0397e+00  3.9195e-01  4.8628e-01  1.9720e-01\n",
      "  7.0788e-02 -5.6636e-01 -7.9355e-02 -1.6658e-01  4.9513e-01  9.4517e-02\n",
      "  3.3129e-02  5.2600e-01 -4.8114e-02  1.0961e-01 -2.0926e-01  5.3785e-01\n",
      " -1.9596e-01 -8.1739e-03 -1.5444e-01 -8.0581e-01 -1.0557e-01 -3.3322e-01\n",
      "  4.6406e-01  9.9518e-02  2.1007e-02 -1.3025e-01  9.2547e-02 -1.7638e-02\n",
      " -1.4717e-01 -1.2243e-01 -6.3543e-02  9.0688e-02  4.7598e-02  1.1890e-01\n",
      " -1.1482e-01 -4.4486e-01 -1.2677e-01 -8.1982e-02  1.4991e-02  6.5389e-01\n",
      "  1.3691e-01 -9.8414e-03 -1.0417e-01  3.8314e-02  9.8986e-04 -5.5078e-01\n",
      "  1.2765e-01 -7.2308e-02  4.9886e-03  2.1618e-01  1.1043e-01  9.3522e-02\n",
      " -5.5075e-02  3.1022e-01 -2.4428e-01 -2.8605e-01  1.9502e-01  9.3397e-02\n",
      " -2.2314e-01 -3.7717e-02  7.8812e-02 -1.3824e-01  1.9336e-02  5.7978e-02\n",
      " -2.4335e-01  2.4662e-01 -3.2615e-01 -6.5513e-02 -2.8090e-01 -8.1939e-03\n",
      "  8.4427e-02  7.6577e-02  9.4961e-02 -5.1576e-01  2.7032e-01  4.0810e-01\n",
      "  5.9457e-01  1.2561e-01 -9.8435e-02 -2.7894e-01  6.2252e-02  1.7876e-01\n",
      "  5.0321e-02  1.2027e-03  1.3166e-01 -9.9114e-02 -4.2881e-01 -7.4613e-02\n",
      " -1.6439e-01  2.6544e-01  3.6246e-01 -4.1890e-01 -2.4099e-01  3.7963e-02\n",
      "  2.6231e-01 -2.2149e-01 -2.2201e-01 -2.7069e-01 -3.1194e-02  3.0734e-01\n",
      "  3.7519e-01  7.3105e-02 -9.4563e-02  2.5243e-01  1.8636e-01  8.4880e-02\n",
      " -5.3694e-01 -3.4793e-01  1.8903e-01  3.0854e-01 -3.1863e-01  1.5979e-01\n",
      " -3.7042e-02 -1.2272e-01 -4.6573e-02  4.7312e-02  4.5770e-01  1.3056e-01\n",
      " -9.8451e-02  3.8605e-01  1.9225e-01  2.8049e-01  1.3640e-01 -1.5697e-01\n",
      " -8.0680e-02  1.0271e-01 -8.8975e-02  4.8581e-02  2.7049e-01  4.9596e-01\n",
      " -5.7315e-02 -1.6473e-01 -5.2998e-01 -7.2100e-01 -4.1816e-01  8.3745e-02\n",
      "  2.9744e-01 -9.6166e-02 -7.4129e-02  6.2675e-01  3.8443e-01  3.2924e-01\n",
      "  7.5049e-02  1.2777e-01 -2.4787e-02  1.1350e-01  8.8798e-02 -3.4373e-01\n",
      "  3.8127e-01 -1.1508e-01 -3.0752e-01  1.8114e-01  2.1235e-02  1.2783e-01\n",
      "  3.5292e-01  5.8156e-01  3.6282e-01 -2.7799e-01  3.1229e-01 -2.8331e-01]\n",
      "the [-6.5276e-01  2.3873e-01 -2.3325e-01  1.8608e-01  3.7674e-01 -5.4116e-02\n",
      " -1.9189e-01  2.2731e-01 -9.2528e-02  1.8388e+00 -1.8715e-01 -2.2237e-01\n",
      "  3.1873e-01  1.1472e-01  3.8304e-01  2.0092e-01 -2.7932e-01  2.3462e+00\n",
      " -3.9846e-01 -1.9525e-01 -2.5649e-01  2.5508e-01  9.4618e-02 -4.1082e-01\n",
      " -3.4191e-01 -1.9499e-01  1.7814e-01  5.3463e-03 -4.7565e-01  2.8022e-01\n",
      " -2.1920e-01  6.0433e-01  2.9309e-01 -2.4232e-01  5.2700e-01  3.9024e-01\n",
      " -5.6955e-01  3.7620e-01 -2.3126e-01 -2.9921e-01  4.5643e-02  1.4555e-01\n",
      "  1.4231e-01  1.0587e-01  4.1210e-01 -2.5261e-01 -3.2090e-02 -5.2830e-01\n",
      " -2.6925e-02  2.6227e-01  1.6375e-01  9.9259e-02  3.1664e-01 -1.1040e-01\n",
      "  2.5732e-01 -4.0720e-01 -6.9903e-02 -1.3189e-01 -5.5753e-01 -1.4815e-01\n",
      " -3.3673e-01 -3.6122e-01  2.1905e-02  6.8589e-01 -8.0151e-02 -1.2327e-01\n",
      " -5.0595e-02 -1.3694e-01  2.7306e-01 -1.4885e-01  4.5150e-03  1.6709e-01\n",
      "  8.8148e-02 -1.7480e-01 -1.2441e-01  5.6394e-02  1.4228e-01  3.6865e-01\n",
      "  5.9486e-02 -1.9471e-01 -2.5575e-01  2.2853e-01 -8.2876e-03  2.0285e-02\n",
      "  4.2212e-01  1.3516e-01  7.2635e-01  3.8426e-01  1.9571e-01 -1.8199e-01\n",
      "  3.6326e-01 -5.9009e-01  1.2660e-01 -2.7157e-01 -4.0160e-01 -6.2389e-01\n",
      "  7.8656e-01  2.8767e-01  5.0652e-01  9.7881e-02 -1.4708e-02  2.3946e-01\n",
      " -1.8723e-01  4.1445e-01  1.6650e-01 -1.8899e+00  3.5874e-01 -2.9300e-02\n",
      " -7.1390e-02  7.2308e-02  6.5537e-02 -1.5800e-01  8.0230e-02 -3.0899e-02\n",
      "  1.9015e-01 -1.4525e-02  9.2995e-03 -4.5801e-01 -5.1586e-02 -4.5419e-02\n",
      "  3.9683e-01 -2.5542e-01  2.4292e-01 -3.0690e-01 -2.4360e-01 -2.7307e-01\n",
      "  1.4416e-01 -9.6950e-03 -5.7948e-01 -2.2177e-01  9.3387e-02 -3.0545e-01\n",
      " -7.1946e-02  2.3493e-01  7.2558e-01  5.4868e-02 -2.7755e-01  1.6177e-01\n",
      " -3.9068e-01 -2.3836e-02 -3.8402e-01  2.9663e-02 -7.9789e-01 -2.6405e-01\n",
      "  5.8656e-02  1.4091e-01  3.0928e-01  1.4004e-01  1.3282e-02  2.1340e-01\n",
      " -3.4682e-01  3.5323e-01  2.9809e-01  9.6521e-02 -2.2186e-02 -1.5502e-01\n",
      " -7.5963e-02  1.0163e+00 -2.0319e-01 -4.8089e-01  1.8526e-03  3.0310e-01\n",
      " -3.0456e-01 -2.9545e-01 -1.1845e-02 -1.5618e-01  2.7693e-01  3.1751e-01\n",
      "  3.6650e-01 -3.9038e-01  4.2004e-01 -3.2852e-01 -6.7994e-02 -1.4249e-01\n",
      " -1.3815e-01 -1.0507e-01 -1.7883e-01 -3.8480e-01  3.2119e-01 -1.8403e-01\n",
      " -5.3652e-02 -2.0844e-02 -7.6724e-01 -1.1748e-01 -7.7480e-02  4.5177e-01\n",
      "  9.6727e-02 -2.0082e-01 -1.9445e-01 -9.7588e-03  3.0302e-01 -3.3607e-01\n",
      "  2.3825e-01 -3.6793e-02 -1.5106e-01 -8.4877e-02 -1.7448e-01 -1.5700e-01\n",
      " -6.5094e-01  1.4706e-02  4.4220e-01  8.0152e-03 -3.4658e-01  2.3566e-01\n",
      "  1.5767e-01 -1.6591e-01 -2.4740e-01 -3.3408e-01 -1.9722e-01  2.2875e-02\n",
      " -1.4465e-01 -2.1910e-01 -9.2784e-02 -4.1141e-01  2.7569e-01 -5.2462e-02\n",
      " -1.1711e-01  2.3524e-01  1.3615e-01  2.0582e-01  3.3483e-04 -3.5967e-02\n",
      " -4.2995e-01 -3.1407e-01  2.7588e-01 -2.0524e-01 -1.7360e-01 -8.1571e-02\n",
      "  3.2323e-01  2.3840e-01  1.2790e-01 -4.0717e-01  5.8714e-01  3.2730e-01\n",
      "  1.7860e-02  1.1729e-01 -4.7061e-02 -7.8453e-02  1.4081e-01 -1.5994e-01\n",
      " -1.4909e-01  7.1805e-02 -3.7698e-02  4.4769e-01  1.6152e-01 -8.2816e-02\n",
      " -1.1207e-01 -3.8554e-02 -1.0233e-01  6.8006e-02  1.6570e-01 -1.8337e-01\n",
      "  3.8589e-01  5.7057e-03 -1.4620e-01  1.2894e+00  3.4140e-01  1.6715e-01\n",
      "  4.2835e-01 -1.0456e-01 -3.0015e-01 -2.4827e-02  2.5433e-01  1.8072e-01\n",
      "  2.6348e-01 -6.7580e-02  1.7232e-01 -7.1153e-02  1.7512e-01 -2.7813e-02\n",
      "  6.4046e-01 -2.7157e-01 -9.4352e-02 -2.2393e-01  8.9257e-03 -3.7729e-01\n",
      " -4.0433e-02  1.1951e-01 -2.2956e-01  3.5904e-01  4.1923e-02  5.3552e-02\n",
      " -1.4838e-01  5.5642e-01 -1.1883e-01  1.1237e-01  3.3574e-04 -1.5964e-01\n",
      "  1.9147e-01  1.1882e-01 -2.8518e-01 -7.7427e-02  2.2063e-01 -2.9288e-01\n",
      " -5.9751e-01 -1.6613e-02 -2.1909e-02 -4.2636e-01  4.8578e-01 -2.8969e-01]\n",
      "king [-6.0644e-01 -5.1205e-01  6.4921e-03 -2.9194e-01 -5.6515e-01 -1.1523e-01\n",
      "  7.7274e-02  3.3561e-01  1.1593e-01  2.3516e+00  5.1773e-02 -5.4229e-01\n",
      " -5.7972e-01  1.3220e-01  2.8430e-01 -7.9592e-02 -2.6762e-01  1.8301e-01\n",
      " -4.1264e-01  2.0459e-01  1.4436e-01 -1.8714e-01 -3.1393e-01  1.7821e-01\n",
      " -1.0997e-01 -2.5584e-01 -1.1149e-01  9.6212e-02 -1.6168e-01  4.0055e-01\n",
      " -2.6115e-01  5.3777e-01 -5.2382e-01  2.7637e-01  7.2191e-01  6.0405e-02\n",
      " -1.7922e-01  1.8020e-01 -1.4381e-01 -1.4795e-01 -8.1394e-02  5.8282e-02\n",
      "  2.2964e-02 -2.6374e-01  1.0704e-01 -4.5425e-01 -1.9964e-01  3.7720e-01\n",
      " -9.7784e-02 -3.1999e-01 -7.8509e-02  6.1502e-01  7.1643e-02 -3.0930e-02\n",
      "  2.1508e-01  2.5280e-01 -3.1643e-01  6.6698e-01  1.9813e-02 -3.2311e-01\n",
      "  2.9266e-02 -4.1403e-02  2.8346e-01 -7.9143e-01  1.3327e-01  7.7231e-02\n",
      " -1.8724e-01 -3.3146e-01 -2.0797e-01 -6.9326e-01 -2.3412e-01 -6.8752e-02\n",
      "  3.8252e-02 -3.2459e-01 -8.3609e-03  1.2945e-01 -2.8316e-01 -5.7546e-01\n",
      "  2.4336e-01  5.6433e-01 -7.1285e-01 -5.4738e-03 -2.3305e-01 -7.1578e-02\n",
      "  4.8301e-01 -3.4312e-01  2.7365e-01 -1.1771e+00 -6.5800e-01 -1.9009e-01\n",
      "  7.4287e-03  3.2977e-01 -1.6647e-01  2.6851e-01  1.1811e-01 -6.2440e-02\n",
      " -4.9987e-02  7.1011e-04 -5.6201e-02 -2.6696e-01  3.1351e-01  4.3955e-01\n",
      " -8.8727e-02 -1.2315e-01  1.8855e-01 -1.0834e+00 -3.3041e-01  5.7325e-01\n",
      " -3.9947e-01  1.4852e-02 -3.6787e-01  3.7842e-01 -2.8962e-01 -7.0543e-02\n",
      " -5.8699e-02  5.3076e-01 -1.2736e-01 -3.5724e-01 -1.5007e-01  1.3823e-02\n",
      " -1.9497e-01 -3.7189e-01  2.6255e-01 -7.6826e-02  8.4217e-02 -5.3640e-01\n",
      "  1.7393e-01 -1.4698e-01 -1.1068e-01  1.7709e-01 -3.9556e-01  1.0433e-01\n",
      "  9.2675e-03 -1.2282e-01 -3.9842e-01 -2.7758e-01 -6.9369e-01  7.0128e-02\n",
      "  8.2794e-02  4.8342e-02 -2.7038e+00 -1.6812e-01  3.1413e-01  2.4313e-02\n",
      " -3.6423e-02  1.9292e-01  4.4872e-01 -4.5427e-01 -3.7271e-01 -9.9532e-01\n",
      " -1.3411e-01 -6.0312e-01  1.6642e-01 -2.4611e-02  6.6891e-01  6.3476e-02\n",
      " -1.1327e+00 -3.3786e-01 -1.2576e-02  3.5344e-01  2.6643e-01 -1.9404e-01\n",
      " -1.9516e-01  6.3670e-01  2.1373e-01 -2.8936e-01 -6.8847e-02 -1.9738e-01\n",
      " -3.5305e-01  1.0219e-01  1.1744e-01  3.7159e-02  4.1041e-01 -1.3766e-02\n",
      " -1.0325e-02  1.0461e-02  3.0697e-02 -3.3016e-01  2.4668e-01 -2.6058e-01\n",
      "  2.8665e-01 -7.8507e-02  6.8945e-03  1.0980e-01 -6.4179e-01  2.4617e-03\n",
      " -2.4693e-01 -1.1188e-02  3.0838e-01  4.5557e-01 -6.2189e-01  1.4873e-01\n",
      "  3.5440e-01  2.8642e-01 -2.4211e-01 -1.2404e-01  2.3326e-01  1.9555e-01\n",
      " -1.2425e-02  1.9920e-01 -1.7935e-01  5.2031e-01 -4.3666e-01  8.6211e-02\n",
      "  1.7282e-01  6.5266e-02  2.8701e-01  6.0238e-01  3.1843e-01 -4.7646e-01\n",
      " -2.1181e-02 -2.7726e-01  4.0253e-01  3.9968e-01  1.8580e-02 -6.2663e-01\n",
      "  3.4149e-01  4.4687e-01 -4.6135e-01  4.4174e-01 -5.7541e-02 -1.9038e-02\n",
      " -2.2626e-01  5.8452e-02 -4.6681e-02 -5.3295e-03 -1.8257e-03  4.8565e-01\n",
      " -4.6144e-01 -4.5877e-01 -1.5891e-01  1.3037e-01 -2.9183e-01  6.9206e-02\n",
      " -4.9825e-02  5.5077e-01  1.4730e-01 -1.9255e-01 -2.3916e-01 -1.9319e-01\n",
      "  1.5643e-01  3.3491e-01 -3.1913e-01  2.0674e-01  6.4556e-02 -2.3195e-01\n",
      "  1.2657e-01 -2.5131e-03  1.1079e-01  3.0436e-01  6.9529e-02  1.1027e-01\n",
      "  2.6285e-01 -2.3103e-01 -2.8933e-01 -5.0675e-02 -8.9796e-02  2.5816e-01\n",
      " -8.0917e-02  3.3160e-01 -3.5930e-01  2.8336e-01  1.4145e-01  2.9012e-01\n",
      "  1.5677e-01  1.3225e-01 -5.0090e-01  2.2110e-01  6.9609e-01 -9.6917e-02\n",
      " -2.4966e-02 -2.9391e-01 -3.1240e-01 -3.8031e-01 -2.0604e-01  1.5959e-01\n",
      " -5.6155e-01  2.9170e-01 -5.0459e-01  6.5684e-02  5.8594e-01  1.3003e-02\n",
      "  6.5874e-01 -4.7811e-01  2.8794e-01  3.5918e-01  4.3347e-01 -4.2480e-01\n",
      "  3.5892e-01 -6.0925e-01 -7.1236e-01  2.9490e-01 -2.1479e-01  2.5658e-01\n",
      " -1.9358e-01  1.1057e+00  2.2862e-01  2.1859e-01 -1.9044e-01 -1.0253e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sen1 = \"the king is great\"\n",
    "sen2 = \"man is the king\"\n",
    "\n",
    "doc1 = nlp(sen1)\n",
    "doc2 = nlp(sen2)\n",
    "\n",
    "print(\"DOC1\")\n",
    "for token in doc1:\n",
    "    print(f\"{token.text} {token.vector}\")\n",
    "    \n",
    "print(\"DOC2\")\n",
    "for token in doc2:\n",
    "    print(f\"{token.text} {token.vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAST TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique wordw : 33\n",
      "Vector Size 128\n",
      "fastext [-6.1595975e-04  4.1009593e-04  9.9575933e-05  2.3051300e-03\n",
      "  7.6282356e-04  5.5791030e-04 -7.6918374e-04 -6.7022134e-04\n",
      "  6.9766771e-04 -7.1074656e-04]..\n",
      "Similar to 'niece' : [('a', 0.21517634391784668), ('room', 0.09632600843906403), ('ten', 0.08491074293851852), ('dursleys', 0.06832265108823776), ('there', 0.05814426392316818)]\n"
     ]
    }
   ],
   "source": [
    "introdu_doc = \"\"\"N early ten years had passed since the Dursleys had woken up to find their\n",
    "nephew on the front step, but Privet Drive had hardly changed at all. The sun\n",
    "rose on the same tidy front gardens and lit up the brass number four on the\n",
    "Dursleys front door; it crept into their living room, which was almost exactly\n",
    "the same as it had been on the night when Mr. Dursley had seen that fateful news\n",
    "report about the owls. Only the photographs on the mantelpiece really showed\n",
    "how much time had passed. Ten years ago, there had been lots of pictures of\n",
    "what looked like a large pink beach ball wearing different-colored bonnets —\n",
    "but Dudley Dursley was no longer a baby, and now the photographs showed a\n",
    "large blond boy riding his first bicycle, on a carousel at the fair, playing a\n",
    "computer game with his father, being hugged and kissed by his mother. The\n",
    "room held no sign at all that another boy lived in the house, too.\n",
    "Yet Harry Potter was still there, asleep at the moment, but not for long.\n",
    "His Aunt Petunia was awake and it was her shrill voice that made the first noise\n",
    "of the day.\"\"\".splitlines()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText\n",
    "\n",
    "tokens_sen = [word_tokenize(a.lower()) for a in introdu_doc]\n",
    "\n",
    "# print(tokens_sen)\n",
    "\n",
    "\n",
    "model = FastText(\n",
    "    sentences=tokens_sen,\n",
    "    vector_size=128,\n",
    "    window=4,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42,\n",
    "    sg=1\n",
    "\n",
    ")\n",
    "\n",
    "ftext = model.wv\n",
    "\n",
    "print(f\"number of unique wordw : {len(ftext.index_to_key)}\")\n",
    "print(f\"Vector Size {ftext.vector_size}\")\n",
    "print(f\"fastext {ftext[\"fasttext\"][:10]}..\")\n",
    "print(f\"Similar to 'niece' : {ftext.similar_by_word(\"niece\", topn=5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
